%% =============================================================================
%% SOC Estimation — LG HG2 Li-ion Battery
%% Wide Tri-Layered Feed-Forward Neural Network (FFNN)
%% Requires: Deep Learning Toolbox, Statistics and Machine Learning Toolbox
%% =============================================================================
clc; clear; close all;

%% =============================================================================
%% CONFIGURATION
%% Set DATASET_ROOT to the folder containing the temperature subfolders:
%%   10degC, 25degC, 40degC, n10degC, n20degC
%% =============================================================================
DATASET_ROOT = ;
TEMP_SUBDIRS = {'10degC', '25degC', '40degC', 'n10degC', 'n20degC'};


%% =============================================================================
%% STEP 1: LOAD & COMBINE ALL CSV FILES
%% =============================================================================
fprintf('\n=== STEP 1: Loading Data ===\n');
combinedTable = loadAndCombineCSVs(DATASET_ROOT, TEMP_SUBDIRS);
fprintf('Combined dataset: %d rows x %d columns\n', height(combinedTable), width(combinedTable));


%% =============================================================================
%% STEP 2: FEATURE SELECTION & CLEANING
%% =============================================================================
fprintf('\n=== STEP 2: Preparing Features ===\n');
[X, y, yColName] = prepareFeatures(combinedTable);


%% =============================================================================
%% STEP 3: SCALING (0 to 1) & TRAIN/TEST SPLIT (80/20)
%% =============================================================================
fprintf('\n=== STEP 3: Scaling & Splitting ===\n');
[X_train, X_test, y_train, y_test, Xmin, Xmax, yMin, yMax] = scaleAndSplit(X, y, 0.2);
fprintf('Train size: %d  |  Test size: %d\n', size(X_train, 1), size(X_test, 1));


%% =============================================================================
%% STEP 4: BUILD WIDE TRI-LAYERED FFNN
%% Architecture: 3 inputs -> Dense(64,ReLU) -> Dense(64,ReLU) -> Dense(64,ReLU) -> 1 output
%% =============================================================================
fprintf('\n=== STEP 4: Building Model ===\n');
net = buildModel();


%% =============================================================================
%% STEP 5: TRAIN THE MODEL
%% =============================================================================
fprintf('\n=== STEP 5: Training ===\n');

% Transpose for MATLAB convention: features x samples
X_train_T = X_train';
y_train_T = y_train';

trainOpts = trainingOptions('adam', ...
    'MaxEpochs',          10, ...
    'MiniBatchSize',      32, ...
    'ValidationSplit',    0.1, ...
    'Shuffle',            'every-epoch', ...
    'Plots',              'training-progress', ...
    'Verbose',            true, ...
    'VerboseFrequency',   1);

[trainedNet, trainInfo] = trainnet(net, X_train_T, y_train_T, 'mse', trainOpts);

fprintf('Training complete.\n');


%% =============================================================================
%% STEP 6: EVALUATE ON TEST SET
%% =============================================================================
fprintf('\n=== STEP 6: Evaluation ===\n');

X_test_T = X_test';
y_pred_scaled = predict(trainedNet, X_test_T);
y_pred_scaled = y_pred_scaled';   % back to column vector

% Inverse-scale predictions and ground truth
y_pred = y_pred_scaled .* (yMax - yMin) + yMin;
y_true = y_test       .* (yMax - yMin) + yMin;

mae_val  = mean(abs(y_true - y_pred));
mse_val  = mean((y_true - y_pred).^2);
rmse_val = sqrt(mse_val);
ss_res   = sum((y_true - y_pred).^2);
ss_tot   = sum((y_true - mean(y_true)).^2);
r2_val   = 1 - (ss_res / ss_tot);

fprintf('\n--- Test Results ---\n');
fprintf('  MAE  : %.4f\n', mae_val);
fprintf('  MSE  : %.4e\n', mse_val);
fprintf('  RMSE : %.4f\n', rmse_val);
fprintf('  R²   : %.4f\n', r2_val);


%% =============================================================================
%% STEP 7: PLOTS
%% =============================================================================
plotTrainingHistory(trainInfo);
plotPredictionsVsActual(y_true, y_pred, yColName);
showSamplePredictions(X_test, y_pred_scaled, 5);


%% =============================================================================
%% LOCAL FUNCTIONS
%% =============================================================================

function combinedTable = loadAndCombineCSVs(datasetRoot, subdirs)
    % Loads all CSV files from each temperature subfolder and concatenates them.
    allTables = {};

    for i = 1:length(subdirs)
        subdir     = subdirs{i};
        folderPath = fullfile(datasetRoot, subdir);

        if ~isfolder(folderPath)
            fprintf('  Warning: Subfolder not found, skipping: %s\n', folderPath);
            continue;
        end

        csvFiles = dir(fullfile(folderPath, '*.csv'));

        if isempty(csvFiles)
            fprintf('  Warning: No CSV files in: %s\n', folderPath);
            continue;
        end

        fprintf('\nLoading from ''%s'' (%d file(s))...\n', subdir, length(csvFiles));

        for j = 1:length(csvFiles)
            filepath = fullfile(folderPath, csvFiles(j).name);
            tbl = parseLgHg2CSV(filepath);

            if ~isempty(tbl)
                % Tag each row with its source temperature folder
                tbl.source_temp = repmat({subdir}, height(tbl), 1);
                allTables{end+1} = tbl; %#ok<AGROW>
                fprintf('  Loaded: %s — %d rows\n', csvFiles(j).name, height(tbl));
            else
                fprintf('  Skipped (no usable data): %s\n', csvFiles(j).name);
            end
        end
    end

    if isempty(allTables)
        error('No data was loaded. Check DATASET_ROOT and that CSV files exist in the temperature subfolders.');
    end

    combinedTable = vertcat(allTables{:});
end


function tbl = parseLgHg2CSV(filepath)
    % Reads an LG HG2 CSV:
    %   - Strips null characters
    %   - Dynamically locates the header row
    %   - Skips the units row immediately after the header

    tbl = [];

    % Read all raw lines
    fid = fopen(filepath, 'r', 'n', 'UTF-8');
    if fid == -1
        fprintf('  Warning: Cannot open file: %s\n', filepath);
        return;
    end
    rawLines = {};
    while ~feof(fid)
        rawLines{end+1} = fgetl(fid); %#ok<AGROW>
    end
    fclose(fid);

    % Clean null characters and strip whitespace
    cleanedLines = strtrim(strrep(rawLines, char(0), ''));

    % Find the header row dynamically
    headerIdx = -1;
    for i = 1:length(cleanedLines)
        line = cleanedLines{i};
        parts = strsplit(line, ',');
        if contains(line, 'Time Stamp') && ...
           contains(line, 'Voltage')    && ...
           contains(line, 'Current')    && ...
           contains(line, 'Temperature') && ...
           length(parts) > 10
            headerIdx = i;
            break;
        end
    end

    if headerIdx == -1
        [~, fname, ext] = fileparts(filepath);
        fprintf('  Warning: Could not find header in %s%s. Skipping.\n', fname, ext);
        return;
    end

    % Units row is immediately after header — skip it
    unitsIdx  = headerIdx + 1;
    dataLines = [cleanedLines(headerIdx), cleanedLines(unitsIdx+1:end)];

    % Remove blank lines
    dataLines = dataLines(~cellfun(@isempty, dataLines));

    if length(dataLines) < 2
        return;  % Only header, no data rows
    end

    % Write to a temp file and read with readtable for robust parsing
    tmpFile = [tempname, '.csv'];
    fid = fopen(tmpFile, 'w');
    for k = 1:length(dataLines)
        fprintf(fid, '%s\n', dataLines{k});
    end
    fclose(fid);

    try
        opts = detectImportOptions(tmpFile);
        opts.DataLines = [2, Inf];   % row 1 is the header
        tbl = readtable(tmpFile, opts);

        % Drop trailing unnamed/empty columns
        varNames = tbl.Properties.VariableNames;
        dropCols = varNames(startsWith(varNames, 'Var') & ...
                            all(ismissing(tbl{:, startsWith(varNames,'Var')}), 1));
        if ~isempty(dropCols)
            tbl = removevars(tbl, dropCols);
        end
    catch ME
        [~, fname, ext] = fileparts(filepath);
        fprintf('  Warning: Failed to parse %s%s: %s\n', fname, ext, ME.message);
        tbl = [];
    end

    delete(tmpFile);
end


function [X, y, yColName] = prepareFeatures(tbl)
    % Selects Voltage, Current, Temperature as features.
    % Target priority: SOC > Capacity.

    requiredFeatures = {'Voltage', 'Current', 'Temperature'};
    varNames = tbl.Properties.VariableNames;

    % Validate feature columns exist
    for i = 1:length(requiredFeatures)
        if ~ismember(requiredFeatures{i}, varNames)
            error('Missing required column: ''%s''. Available: %s', ...
                  requiredFeatures{i}, strjoin(varNames, ', '));
        end
    end

    % Determine target column
    if ismember('SOC', varNames)
        yColName = 'SOC';
    elseif ismember('Capacity', varNames)
        fprintf('Warning: ''SOC'' not found — using ''Capacity'' as target instead.\n');
        yColName = 'Capacity';
    else
        error('Neither ''SOC'' nor ''Capacity'' found. Available columns: %s', ...
              strjoin(varNames, ', '));
    end

    % Convert to numeric, coercing non-numeric to NaN
    allCols = [requiredFeatures, {yColName}];
    for i = 1:length(allCols)
        col = allCols{i};
        raw = tbl.(col);
        if iscell(raw)
            tbl.(col) = cellfun(@(v) str2double(string(v)), raw);
        elseif ~isnumeric(raw)
            tbl.(col) = str2double(string(raw));
        end
    end

    % Remove rows with any NaN in the relevant columns
    colData = [tbl.Voltage, tbl.Current, tbl.Temperature, tbl.(yColName)];
    validRows = ~any(isnan(colData), 2);
    tbl = tbl(validRows, :);

    if height(tbl) == 0
        error('DataFrame is empty after cleaning. Check your CSV contents.');
    end

    fprintf('Target column   : ''%s''\n', yColName);
    fprintf('Rows after clean: %d\n', height(tbl));

    X = [tbl.Voltage, tbl.Current, tbl.Temperature];
    y = tbl.(yColName);
end


function [X_train, X_test, y_train, y_test, Xmin, Xmax, yMin, yMax] = ...
         scaleAndSplit(X, y, testFraction)
    % Min-Max scales X and y to [0,1], then splits 80/20.

    % Scale X column-wise
    Xmin = min(X, [], 1);
    Xmax = max(X, [], 1);
    X_scaled = (X - Xmin) ./ (Xmax - Xmin + eps);

    % Scale y
    yMin = min(y);
    yMax = max(y);
    y_scaled = (y - yMin) ./ (yMax - yMin + eps);

    % Reproducible random split
    rng(42);
    n        = size(X_scaled, 1);
    nTest    = round(testFraction * n);
    idx      = randperm(n);
    testIdx  = idx(1:nTest);
    trainIdx = idx(nTest+1:end);

    X_train = X_scaled(trainIdx, :);
    X_test  = X_scaled(testIdx,  :);
    y_train = y_scaled(trainIdx);
    y_test  = y_scaled(testIdx);
end


function net = buildModel()
    % Wide Tri-Layered FFNN matching the architecture in the paper.
    % Input: 3 features (Voltage, Current, Temperature)
    % Hidden layers: 3 x Dense(64, ReLU)
    % Output: 1 neuron (SOC), linear activation

    net = dlnetwork();

    layers = [
        featureInputLayer(3,        'Name', 'input')
        fullyConnectedLayer(64,     'Name', 'fc1')
        reluLayer(                  'Name', 'relu1')
        fullyConnectedLayer(64,     'Name', 'fc2')
        reluLayer(                  'Name', 'relu2')
        fullyConnectedLayer(64,     'Name', 'fc3')
        reluLayer(                  'Name', 'relu3')
        fullyConnectedLayer(1,      'Name', 'output')
        regressionLayer(            'Name', 'regressionOutput')
    ];

    net = layerGraph(layers);

    fprintf('Model architecture built:\n');
    fprintf('  Input(3) -> Dense(64,ReLU) -> Dense(64,ReLU) -> Dense(64,ReLU) -> Output(1,Linear)\n');
end


function plotTrainingHistory(trainInfo)
    % Plots training and validation loss (MSE) over epochs.
    figure('Name', 'Training History', 'NumberTitle', 'off');

    epochs = 1:length(trainInfo.TrainingLoss);

    subplot(1,2,1);
    plot(epochs, trainInfo.TrainingLoss, 'b-', 'LineWidth', 1.5); hold on;
    if isfield(trainInfo, 'ValidationLoss') && ~isempty(trainInfo.ValidationLoss)
        valEpochs = find(~isnan(trainInfo.ValidationLoss));
        plot(valEpochs, trainInfo.ValidationLoss(valEpochs), 'r--', 'LineWidth', 1.5);
        legend('Train Loss', 'Val Loss');
    else
        legend('Train Loss');
    end
    xlabel('Epoch'); ylabel('MSE Loss');
    title('Loss (MSE)'); grid on;

    subplot(1,2,2);
    plot(epochs, trainInfo.TrainingRMSE, 'b-', 'LineWidth', 1.5); hold on;
    if isfield(trainInfo, 'ValidationRMSE') && ~isempty(trainInfo.ValidationRMSE)
        valEpochs = find(~isnan(trainInfo.ValidationRMSE));
        plot(valEpochs, trainInfo.ValidationRMSE(valEpochs), 'r--', 'LineWidth', 1.5);
        legend('Train RMSE', 'Val RMSE');
    else
        legend('Train RMSE');
    end
    xlabel('Epoch'); ylabel('RMSE');
    title('Root Mean Squared Error'); grid on;

    saveas(gcf, 'training_history.png');
    fprintf('Training history plot saved to training_history.png\n');
end


function plotPredictionsVsActual(y_true, y_pred, yColName)
    % Scatter plot of predicted vs actual SOC values.
    figure('Name', 'Predictions vs Actual', 'NumberTitle', 'off');
    scatter(y_true, y_pred, 5, 'filled', 'MarkerFaceAlpha', 0.3); hold on;
    refLine = linspace(min(y_true), max(y_true), 100);
    plot(refLine, refLine, 'r--', 'LineWidth', 1.5);
    xlabel(['Actual ', yColName]);
    ylabel(['Predicted ', yColName]);
    title('Predicted vs Actual SOC');
    legend('Predictions', 'Perfect fit', 'Location', 'northwest');
    grid on;
    saveas(gcf, 'predictions_vs_actual.png');
    fprintf('Predictions plot saved to predictions_vs_actual.png\n');
end


function showSamplePredictions(X_test_scaled, y_pred_scaled, n)
    % Prints the first n scaled predictions alongside scaled features.
    fprintf('\nFirst %d Scaled Predictions:\n', n);
    fprintf('%-12s %-12s %-12s %-20s\n', ...
            'V_scaled', 'I_scaled', 'T_scaled', 'Predicted_SOC_scaled');
    fprintf('%s\n', repmat('-', 1, 58));
    for i = 1:min(n, size(X_test_scaled, 1))
        fprintf('%-12.4f %-12.4f %-12.4f %-20.4f\n', ...
                X_test_scaled(i,1), X_test_scaled(i,2), ...
                X_test_scaled(i,3), y_pred_scaled(i));
    end
end